{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc440bd9-e22a-4280-bcb4-e3da752b222e",
   "metadata": {},
   "source": [
    "#### **ideas & problems**\n",
    "- add \"verbose\" option(dev)\n",
    "- change the \"create_texts\" function  \n",
    "  - to avoid re-runs (dev)\n",
    "  - to get metadata about the sections\n",
    "- make analysis of the full corpus, and compare it with the specific selection.\n",
    "- find a way to remove meaningless abreviations like \"(kbs)\"\n",
    "  - maybe check if words are in \"ubuntu_eng-words-huge\"\n",
    "- play with LDA's parameters\n",
    "- use Tf–idf to remove least meaningfull words\n",
    "- remove redundant versions of the same article. \"v1,v2,v3\" etc...\n",
    "- remover palavras chave das frazes.\n",
    "  \n",
    "#### **INFOS**\n",
    "- vect -> contains the count_vectorizer model parameters, including **the n-gram's vocabulary**\n",
    "- vects -> contains the document-term matrix itself\n",
    "- model -> contains the results of the LDA algorithm, including:\n",
    "  - document-topic matrix\n",
    "  - topic-word matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a69f8cd-fa3a-4ae7-a22c-08788d453c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tales\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tales\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import functions as ft\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400a61ea-3ae0-43d4-b6b5-4ec2de60ce52",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft.run_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c53f35-8983-4a6d-a47c-a0ac819dd6a7",
   "metadata": {},
   "source": [
    "## --------------------------------------------\n",
    "## ABAIXO DAQUI - CÓDIGOS DE TESTE\n",
    "## --------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0354469e-cdea-4bd5-b595-c467230154dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9882\n",
      "\n",
      "9882\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Database already has 2175 phrases, do you want to retry query? Y-n n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "glob_paths_ex = [\"./pdf/*/\"]\n",
    "files = ft.list_xmls(glob_paths_ex)\n",
    "test_text = ft.create_texts(files)\n",
    "test_phrases = ft.search_texts(test_text,{\"artificial intelligence\", \"machine learning\", \"m.l.\",\"a.i.\"},{\"abstract\",\"text\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced01143-3d29-4b3b-bec1-1f85e67cd3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_phrases = ft.search_texts(test_text,{\"artificial intelligence\", \"machine learning\", \"m.l.\",\"a.i.\"},{\"abstract\",\"text\"})\n",
    "test_clean = ft.clean_text()\n",
    "(vect,vects) = ft.create_n_gram(test_clean)\n",
    "model = ft.run_lda(vect,vects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc3d4647-7a1c-44cb-a7fe-316cd8422367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "introduction in the field of artificial intelligence, knowledge-based systems (kbs) and behaviour-based systems (bbs) have modelled and simulated exhibitions of intelligence of different types, which we could call \"cognitive\" intelligence and \"adaptive\" intelligence, respectively.\n",
      "introduction field artificial intelligence knowledge-based systems kbs behaviour-based systems bbs modelled simulated exhibitions intelligence different types could call cognitive intelligence adaptive intelligence respectively\n"
     ]
    }
   ],
   "source": [
    "print(test_phrases[465])\n",
    "print(test_clean[465])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37b08999-3307-42fc-ab62-09732c1cc95f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mloglikelihoods_)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(model.loglikelihoods_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4cc2dd-ec3a-422f-bd60-6a6b7f361e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "#i = random.randint(0,1785)\n",
    "i = 712\n",
    "print(i)\n",
    "plt.plot(model.ndz_[i])\n",
    "plt.plot(model.doc_topic_[i])\n",
    "print(model.ndz_[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1bed58-81ca-4c8c-b14a-4f7208d375ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "#i = random.randint(0,1785)\n",
    "i = 1\n",
    "print(i)\n",
    "plt.plot(model.nzw_[i])\n",
    "plt.plot(model.topic_word_[i])\n",
    "print(model.nzw_[i][10:100])\n",
    "print(np.around(model.topic_word_[i][10:100],4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ba72b7-95f8-41fe-92f7-90d3cd542c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get topic words\n",
    "mq_z = (model.nzw_[i] > 0).sum()\n",
    "\n",
    "arroto = (-model.nzw_[i]).argsort()[:mq_z]\n",
    "len(n_words_on_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6b5690-cc70-4831-918d-3874ba47e5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lista de palavras e probabilidades de cada tópico:\n",
    "n_words_on_topic = (model.nzw_ > 0).sum(axis=1)\n",
    "top_word_indices_of_topics = [(-model.nzw_[a]).argsort()[:n_words_on_topic[a]] for a in range(len(n_words_on_topic))]\n",
    "[[(np.around(model.topic_word_,5)[i][word],(vect.get_feature_names())[word]) for word in top_word_indices]for i,top_word_indices in enumerate(top_word_indices_of_topics)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8b27d30-c5bf-4f9f-bada-a682fc4a1b30",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#lista de palavras e probabilidades de cada tópico:\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m n_topics_on_document \u001b[38;5;241m=\u001b[39m (\u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mndz_ \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      3\u001b[0m top_topic_indices_of_documents \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;241m-\u001b[39mmodel\u001b[38;5;241m.\u001b[39mndz_[a])\u001b[38;5;241m.\u001b[39margsort()[:n_topics_on_document[a]] \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(n_topics_on_document))]\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i,top_topic_indices \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(top_topic_indices_of_documents):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "#lista de palavras e probabilidades de cada tópico:\n",
    "n_topics_on_document = (model.ndz_ > 0).sum(axis=1)\n",
    "top_topic_indices_of_documents = [(-model.ndz_[a]).argsort()[:n_topics_on_document[a]] for a in range(len(n_topics_on_document))]\n",
    "for i,top_topic_indices in enumerate(top_topic_indices_of_documents):\n",
    "    print([(topic,np.around(model.doc_topic_,5)[i][topic]) for topic in top_topic_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb1c741-6f9c-48d6-b79d-9f3b2160ce09",
   "metadata": {},
   "outputs": [],
   "source": [
    "[(model.topic_word_[i][blep],(vect.get_feature_names())[blep]) for blep in arroto]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c778b6-732a-4c67-8a0b-891be711327a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46573038-6a74-4ebf-a8a7-f036040634d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd8e4fd-07b5-4039-bf1c-617be564af01",
   "metadata": {},
   "outputs": [],
   "source": [
    "string1 = \"ovon\"\n",
    "string2 = \"ovon de codorna\"\n",
    "print(re.search(r\"\\b\" + re.escape(string1) + r\"\\b\", string2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6f44cf-0546-488e-91be-b40a252603a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "words = set(nltk.corpus.words.words())\n",
    "\n",
    "sent = \"Io andiamo to the beach with my amico.\"\n",
    "\" \".join(w for w in nltk.wordpunct_tokenize(sent) \\\n",
    "         if w.lower() in words or not w.isalpha())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c161868-b264-4e87-b81e-93d201f532b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vects.toarray()\n",
    "np.sum(X,axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01854027-42f3-4e93-8ceb-43c58e160712",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.__dict__)\n",
    "print(dir(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2f5827-d0c0-45d4-9617-9eaed295a435",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import colors\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "a = [len(test_phrase) for test_phrase in test_phrases]\n",
    "\n",
    "n_bins = 150\n",
    "treshold = 500\n",
    "dist1 = a\n",
    "\n",
    "b = [len(test_phrase) for test_phrase in test_phrases if (len(test_phrase) < treshold)]\n",
    "\n",
    "fig, ax = plt.subplots(figsize =(10, 6))\n",
    "\n",
    "plt.ylabel(\"nº of documents ({}% are ommited)\".format(round(((len(a)-len(b))/len(a))*100,1)))\n",
    "plt.xlabel(\"nº of characters (documents with over {} characters are ommited in this graph)\".format(treshold))\n",
    "\n",
    "# N is the count in each bin, bins is the lower-limit of the bin\n",
    "N, bins, patches = ax.hist(dist1, bins=n_bins, range=(min(a), treshold))\n",
    "\n",
    "# We'll color code by height, but you could use any scalar\n",
    "fracs = N / N.max()\n",
    "\n",
    "# we need to normalize the data to 0..1 for the full range of the colormap\n",
    "norm = colors.Normalize(fracs.min(), fracs.max())\n",
    "\n",
    "# Now, we'll loop through our objects and set the color of each accordingly\n",
    "for thisfrac, thispatch in zip(fracs, patches):\n",
    "    color = plt.cm.viridis(norm(thisfrac))\n",
    "    thispatch.set_facecolor(color)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f88163-e6b9-409d-b520-a1703f658502",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
